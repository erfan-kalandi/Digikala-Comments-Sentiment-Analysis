{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"X3gPyGNT6nG3"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from collections import Counter\n","import re\n","import matplotlib.pyplot as plt\n","from hazm.utils import stopwords_list\n","from hazm import Normalizer\n","from tqdm import tqdm\n","from torch.optim.lr_scheduler import StepLR\n","from hazm import Stemmer, WordTokenizer\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import StratifiedShuffleSplit\n","import torch.nn as nn\n"]},{"cell_type":"markdown","metadata":{"id":"XBrv71FW6nG6"},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WY2isJeB6nG8"},"outputs":[],"source":["df = pd.read_csv(\"data.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"To8TKgiu6nG9"},"outputs":[],"source":["if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-mKZcgX6nG_"},"outputs":[],"source":["df.drop(columns=['Score'], axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cVLHBOH26nHA"},"outputs":[],"source":["split = StratifiedShuffleSplit(n_splits=1, test_size=0.25)\n","for train_idx, test_idx in split.split(df[\"Text\"].values, df[\"Suggestion\"].values):\n","    X_train, X_test = df[\"Text\"].values[train_idx], df[\"Text\"].values[test_idx]\n","    y_train, y_test = df[\"Suggestion\"].values[train_idx] - 1, df[\"Suggestion\"].values[test_idx] - 1\n"]},{"cell_type":"markdown","metadata":{"id":"PrdcQ4oX6nHB"},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y9Ln3-8M6nHC"},"outputs":[],"source":["tokenizer = WordTokenizer()\n","stopwords = stopwords_list()\n","normalizer = Normalizer()\n","stemmer = Stemmer()\n","\n","def stem_tokens(tokens):\n","    return [stemmer.stem(token) for token in tokens]\n","\n","def tokenize(text):\n","    return tokenizer.tokenize(text)\n","\n","def remove_stopwords(tokens):\n","    return [token for token in tokens if token not in stopwords]\n","\n","def preprocess_text(text):\n","    text = normalizer.normalize(text)\n","    text = re.sub(r\"[^\\w\\s\\u0600-\\u06FF]\", \"\", text)\n","    text = re.sub(r\"[\\d۰-۹]+\", \"\", text)\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","    tokens = tokenize(text)\n","    tokens = remove_stopwords(tokens)\n","    tokens = stem_tokens(tokens)\n","    return ' '.join(tokens)\n","\n","def preprocess_series(text_series):\n","    return text_series.apply(lambda x: preprocess_text(x))\n","\n","X_train = preprocess_series(pd.Series(X_train))\n","print('Preprocessing for train done.')\n","\n","X_test = preprocess_series(pd.Series(X_test))\n","print('Preprocessing for test done.')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yu6RT3Bj6nHE"},"outputs":[],"source":["y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n","y_test_tensor = torch.tensor(y_test, dtype=torch.long)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0qrn4dmB6nHG"},"outputs":[],"source":["tokenized_train = X_train.apply(lambda x: x.split())\n","tokenized_test = X_test.apply(lambda x: x.split())\n","\n","all_tokens = [token for tokens in tokenized_train for token in tokens]\n","word_counts = Counter(all_tokens)\n","\n","vocab = {word: idx + 2 for idx, (word, _) in enumerate(word_counts.most_common())}\n","vocab[\"<PAD>\"] = 0\n","vocab[\"<UNK>\"] = 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kcr2do396nHH"},"outputs":[],"source":["def tokens_to_indices(tokens, vocab):\n","    return [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n","\n","X_train_indices = tokenized_train.apply(lambda tokens: tokens_to_indices(tokens, vocab))\n","X_test_indices = tokenized_test.apply(lambda tokens: tokens_to_indices(tokens, vocab))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"88M23Dph6nHI"},"outputs":[],"source":["max_len = 123\n","\n","def pad_seq(seq, max_len):\n","    return seq[:max_len] + [0]*(max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n","\n","X_train_padded = torch.tensor([pad_seq(seq, max_len) for seq in X_train_indices])\n","X_test_padded = torch.tensor([pad_seq(seq, max_len) for seq in X_test_indices])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VL6Be0D86nHI"},"outputs":[],"source":["class CommentDataset(Dataset):\n","    def __init__(self, inputs, labels):\n","        self.inputs = inputs\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.inputs)\n","\n","    def __getitem__(self, idx):\n","        return self.inputs[idx], self.labels[idx]\n","\n","train_dataset = CommentDataset(X_train_padded, y_train_tensor)\n","test_dataset = CommentDataset(X_test_padded, y_test_tensor)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZTfRoM7I6nHJ"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class Attention(nn.Module):\n","    def __init__(self, hidden_dim):\n","        super(Attention, self).__init__()\n","        self.attn = nn.Linear(hidden_dim * 2, 1)\n","\n","    def forward(self, lstm_output):\n","        attn_weights = torch.softmax(self.attn(lstm_output), dim=1)\n","        context = torch.sum(attn_weights * lstm_output, dim=1)\n","        return context\n","\n","class TextClassificationModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, padding_idx=0):\n","        super(TextClassificationModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n","        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)\n","        self.attention = Attention(hidden_dim)\n","        self.dropout = nn.Dropout(0.5)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_output, _ = self.lstm(embedded)\n","        context = self.attention(lstm_output)\n","        output = self.fc(self.dropout(context))\n","        return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A5CrSGrd6nHK"},"outputs":[],"source":["embedding_matrix = np.random.normal(0, 1, (len(vocab), max_len)).astype(np.float32)\n","\n","vocab_size = len(vocab)\n","hidden_dim = 16\n","output_dim = 3\n","\n","model = TextClassificationModel(vocab_size, max_len, hidden_dim, output_dim)\n","model.to(device)\n","print(model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hGDTtZeC6nHK"},"outputs":[],"source":["n_epochs = 40\n","val_per_epoch = 0\n","optimizer = optim.Adam(model.parameters(), lr=0.0002)\n","\n","class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n","class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n","\n","criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRFEUveB6nHK"},"outputs":[],"source":["train_loss = list()\n","\n","class EarlyStopping:\n","    def __init__(self, patience=5, verbose=False):\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_loss = None\n","        self.early_stop = False\n","\n","    def __call__(self, val_loss):\n","        if self.best_loss is None or val_loss < self.best_loss:\n","            self.best_loss = val_loss\n","            self.counter = 0\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","                if self.verbose:\n","                    print(\"Early stopping\")\n","\n","    def should_stop(self):\n","        return self.early_stop\n","\n","\n","early_stopping = EarlyStopping(patience=3, verbose=True)\n","\n","\n","for epoch in range(n_epochs):\n","    running_loss = 0\n","    running_acc = 0\n","\n","    model.train()\n","    for idx, (X, y) in enumerate(tqdm(train_loader)):\n","        optimizer.zero_grad()\n","        X, y = X.to(device), y.to(device)\n","        y_hat = model(X)\n","        loss = criterion(y_hat, y)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    running_loss = running_loss/ len(train_loader)\n","    running_acc = running_acc / len(train_loader)\n","    train_loss.append(running_loss)\n","    print(f'Epoch {epoch+1}/{n_epochs} : training loss: {round(running_loss,3)}')\n","\n","    early_stopping(running_loss)\n","    if early_stopping.should_stop():\n","      print(\"Early stopping triggered\")\n","      break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vaJc7kUx6nHL"},"outputs":[],"source":["plt.title('train loss')\n","plt.plot(train_loss)\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"loss\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wmPc9Pvs6nHL"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(X_test_padded.to(device))\n","    _, predicted_labels = torch.max(outputs, 1)\n","    predicted_labels = predicted_labels.cpu().numpy()\n","    true_labels = y_test_tensor.cpu().numpy()\n","\n","print(\"Accuracy:\", accuracy_score(true_labels, predicted_labels))\n","print(\"Precision:\", precision_score(true_labels, predicted_labels, average='weighted'))\n","print(\"Recall:\", recall_score(true_labels, predicted_labels, average='weighted'))\n","print(\"F1 Score:\", f1_score(true_labels, predicted_labels, average='weighted'))\n","\n","print(\"\\nClassification Report:\")\n","print(classification_report(true_labels, predicted_labels))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"al3oXefY6nHM"},"outputs":[],"source":["print(\"\\nSample predictions on test data:\")\n","for i in range(7):\n","    print(f\"Text: {X_test[i]}\")\n","    print(f\"True Label: {true_labels[i]+1}, Predicted: {predicted_labels[i]+1}\")\n","    print(\"------\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}